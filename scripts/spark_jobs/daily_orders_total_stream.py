# scripts/spark_jobs/daily_orders_total_stream.py
import os
import logging
from typing import Optional

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType, TimestampType, DateType
)
from pyspark.sql.functions import col, to_timestamp, to_date, sum as sum_, lit

from plugins.parser import parse_args
from plugins.writer import write_to_bigquery  # <- your helper (append, direct)

# ---------------- Spark session ----------------
def build_spark(app_name: str,
                project: Optional[str],
                temp_gcs_bucket: Optional[str],
                session_tz: str = "Europe/Bucharest") -> SparkSession:
    """
    Build SparkSession with BigQuery connector configs.
    Use Europe/Bucharest so the 'day' matches your local logic.
    """
    builder = (
        SparkSession.builder
            .appName(app_name)
            .config("spark.sql.shuffle.partitions", "200")
            .config("spark.sql.session.timeZone", session_tz)
    )
    if project:
        builder = builder.config("spark.bigquery.projectId", project)
    if temp_gcs_bucket:
        builder = builder.config("spark.bigquery.tempGcsBucket", temp_gcs_bucket)
    return builder.getOrCreate()

# ---------------- Schema & source ----------------
def orders_schema() -> StructType:
    """ CSV schema generated by your simulator. """
    return StructType([
        StructField("order_id", StringType(), True),
        StructField("quantity", IntegerType(), True),
        StructField("type", StringType(), True),
        StructField("date", TimestampType(), True),  # event time
    ])

def read_orders_stream_for_day(spark: SparkSession, bucket: str, day_str: str) -> DataFrame:
    """
    Read only files for the requested day:
      gs://<bucket>/orders/date=YYYY-MM-DD/*.csv
    """
    path = f"gs://{bucket}/orders/date={day_str}/*.csv"
    return (
        spark.readStream
             .schema(orders_schema())
             .option("header", True)
             .csv(path)
    )

# ---------------- Cleaning (event time) & dedup ----------------
def clean_filter_dedup_for_day(orders_stream: DataFrame, day_str: str) -> DataFrame:
    """
    - Parse event time to 'ts'
    - Keep only valid rows
    - Keep only the requested day (event_date == --date)
    - Watermark on TIMESTAMP ('ts') + dropDuplicates(order_id)
    """
    base = (
        orders_stream
            .withColumn("ts", to_timestamp(col("date")))
            .where(
                col("order_id").isNotNull()
                & col("type").isNotNull()
                & col("quantity").isNotNull()
                & (col("quantity") >= 0)
                & col("ts").isNotNull()
            )
            .drop("date")
            .withColumn("event_date", to_date(col("ts")))
            .where(col("event_date") == lit(day_str))
            .select("event_date", "type", "order_id", "quantity", "ts")
    )

    dedup = (
        base
            .withWatermark("ts", "1 day")
            .dropDuplicates(["order_id"])
            .select("event_date", "type", "quantity")
    )
    return dedup

# ---------------- Main (continuous) ----------------
def main():
    """
    Args (from DAG):
      --date         : target day (YYYY-MM-DD)
      --project      : GCP project
      --dataset      : BigQuery dataset
      --table        : BigQuery table (e.g., daily_sales)
      --temp-bucket  : GCS temp bucket for the BigQuery connector
    Env:
      GCS_BUCKET     : bucket where orders/date=YYYY-MM-DD/*.csv land
      CHECKPOINT_DIR : optional (derived if missing)
    """
    args = parse_args(["date", "project", "dataset", "table", "temp-bucket"])

    bucket = os.getenv("GCS_BUCKET")
    if not bucket:
        raise RuntimeError("Env var GCS_BUCKET must be set")

    day_str = args.date
    checkpoint_dir = os.getenv("CHECKPOINT_DIR") or f"gs://{bucket}/checkpoints/orders_daily/{day_str}/"

    spark = build_spark(
        app_name=f"orders_daily_total_stream_{day_str}",
        project=args.project,
        temp_gcs_bucket=args.temp_bucket,
        session_tz="Europe/Bucharest",
    )
    logging.getLogger().setLevel(logging.INFO)

    # 1) Source: only that day
    day_stream = read_orders_stream_for_day(spark, bucket, day_str)

    # 2) Clean + dedup for that day
    day_rows = clean_filter_dedup_for_day(day_stream, day_str)

    # BigQuery table FQN
    table_fqn = f"{args.project}.{args.dataset}.{args.table}"

    def writer_fn(batch_df: DataFrame, batch_id: int):
        """
        For each micro-batch:
          - aggregate the *increment* seen in this batch
          - append to BigQuery (WRITE_APPEND) using your helper
          - log what we wrote
        """
        # Aggregate increment: one row per (event_date, type)
        out_df = (
            batch_df.groupBy("event_date", "type")
                    .agg(sum_("quantity").alias("total_quantity"))
                    .withColumn("event_date", col("event_date").cast(DateType()))
        )

        # If empty, nothing to write
        if out_df.rdd.isEmpty():
            logging.info("[daily_orders_total_stream][batch %s] No new rows for day %s.", batch_id, day_str)
            return

        # Log preview
        cnt = out_df.count()
        logging.info("[daily_orders_total_stream][batch %s] Writing %d row(s) to %s", batch_id, cnt, table_fqn)
        out_df.orderBy("type").show(n=min(50, cnt), truncate=False)

        # Append to BigQuery using your existing helper (direct + append)
        write_to_bigquery(out_df, table_fqn)

        logging.info("[daily_orders_total_stream][batch %s] BigQuery append done.", batch_id)

    # 3) Continuous streaming: write whenever new files arrive
    query = (
        day_rows.writeStream
                .outputMode("append")                  # we pass raw (deduped) rows to foreachBatch
                .foreachBatch(writer_fn)               # inside, we aggregate and append
                .option("checkpointLocation", checkpoint_dir)
                .trigger(processingTime="15 seconds")  # run every 15s; waits for new files
                .start()
    )

    query.awaitTermination()

if __name__ == "__main__":
    main()
